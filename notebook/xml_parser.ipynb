{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory: /Users/howechen/Project/ntu_sd6103_team_project/ntu_sd6103_data_systems_team_project\n"
     ]
    }
   ],
   "source": [
    "if os.getcwd().endswith('notebook'):\n",
    "    os.chdir('../')\n",
    "print(f'current working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 1: {'type': 'proceedings', 'id': 'reference/med/2013', 'mdate': '2017-05-16', 'editor': 'Ankur Agarwal', 'title': 'Handbook of Medical and Healthcare Technologies', 'year': '2013', 'publisher': 'Springer', 'ee': 'https://doi.org/10.1007/978-1-4614-8495-0', 'isbn': '978-1-4614-8494-3', 'url': 'db/reference/med/med2013.html'}\n",
      "Record 2: {'type': 'proceedings', 'id': 'reference/genetic/2015', 'mdate': '2020-03-27', 'editor': 'Conor Ryan', 'title': 'Handbook of Genetic Programming Applications', 'publisher': 'Springer', 'year': '2015', 'isbn': '978-3-319-20882-4', 'ee': 'https://doi.org/10.1007/978-3-319-20883-1', 'url': 'db/reference/genetic/genetic2015.html'}\n",
      "Record 3: {'type': 'proceedings', 'id': 'series/isrl/97', 'mdate': '2017-05-16', 'editor': 'Seda Yanik', 'title': 'Intelligent Decision Making in Quality Management - Theory and Applications', 'publisher': 'Springer', 'year': '2016', 'series': 'Intelligent Systems Reference Library', 'volume': '97', 'isbn': '978-3-319-24497-6', 'ee': 'https://doi.org/10.1007/978-3-319-24499-0', 'url': 'db/series/isrl/isrl97.html'}\n",
      "Record 4: {'type': 'proceedings', 'id': 'series/sapere/2013-5', 'mdate': '2019-09-06', 'editor': 'ller', 'title': 'Philosophy and Theory of Artificial Intelligence, PT-AI 2011, Thessaloniki, Greece, October 3-4, 2011, Proceedings.', 'booktitle': 'PT-AI', 'year': '2013', 'publisher': 'Springer', 'isbn': '978-3-642-31673-9', 'series': 'Studies in Applied Philosophy, Epistemology and Rational Ethics', 'volume': '5', 'ee': 'https://doi.org/10.1007/978-3-642-31674-6', 'url': 'db/series/sapere/sapere5.html'}\n",
      "Record 5: {'type': 'proceedings', 'id': 'series/ssw/49', 'mdate': '2021-04-19', 'editor': 'Gian Luca Pozzato', 'title': 'Applications and Practices in Ontology Design, Extraction, and Reasoning.', 'publisher': 'IOS Press', 'year': '2020', 'booktitle': 'Applications and Practices in Ontology Design, Extraction, and Reasoning', 'series': 'Studies on the Semantic Web', 'volume': '49', 'isbn': '978-1-64368-143-6', 'ee': 'https://doi.org/10.3233/SSW49', 'url': 'db/series/ssw/ssw49.html'}\n",
      "Record 6: {'type': 'proceedings', 'id': 'series/faia/2007-160', 'mdate': '2022-03-31', 'editor': 'John Soldatos 0001', 'title': 'Emerging Artificial Intelligence Applications in Computer Engineering - Real Word AI Systems with Applications in eHealth, HCI, Information Retrieval and Pervasive Technologies', 'booktitle': 'Emerging Artificial Intelligence Applications in Computer Engineering', 'volume': '160', 'series': 'Frontiers in Artificial Intelligence and Applications', 'year': '2007', 'isbn': '978-1-58603-780-2', 'publisher': 'IOS Press', 'url': 'db/series/faia/faia160.html'}\n",
      "Record 7: {'type': 'proceedings', 'id': 'series/faia/2008-181', 'mdate': '2009-02-17', 'editor': 'Andrzej Sieminski', 'title': 'New Trends in Multimedia and Network Information Systems', 'booktitle': 'New Trends in Multimedia and Network Information Systems', 'volume': '181', 'series': 'Frontiers in Artificial Intelligence and Applications', 'year': '2008', 'isbn': '978-1-58603-904-2', 'publisher': 'IOS Press', 'url': 'db/series/faia/faia181.html'}\n",
      "Record 8: {'type': 'proceedings', 'id': 'series/faia/2006-149', 'mdate': '2009-02-26', 'editor': 'Robert J. Howlett', 'title': 'Integrated Intelligent Systems for Engineering Design', 'booktitle': 'Integrated Intelligent Systems for Engineering Design', 'volume': '149', 'series': 'Frontiers in Artificial Intelligence and Applications', 'year': '2006', 'isbn': '978-1-58603-675-1', 'publisher': 'IOS Press', 'url': 'db/series/faia/faia149.html'}\n",
      "Record 9: {'type': 'proceedings', 'id': 'series/faia/2008-167', 'mdate': '2016-07-11', 'editor': 'Philipp Cimiano', 'title': 'Ontology Learning and Population: Bridging the Gap between Text and Knowledge', 'booktitle': 'Ontology Learning and Population', 'year': '2008', 'publisher': 'IOS Press', 'series': 'Frontiers in Artificial Intelligence and Applications', 'volume': '167', 'isbn': '978-1-58603-818-2', 'ee': 'http://www.booksonline.iospress.nl/Content/View.aspx?piid=8211', 'url': 'db/series/faia/faia167.html'}\n",
      "Record 10: {'type': 'proceedings', 'id': 'series/faia/2009-196', 'mdate': '2009-09-16', 'editor': 'Alessandro Sperduti', 'title': 'Computational Intelligence and Bioengineering - Essays in Memory of Antonina Starita', 'booktitle': 'Computational Intelligence and Bioengineering', 'volume': '196', 'series': 'Frontiers in Artificial Intelligence and Applications', 'year': '2009', 'isbn': '978-1-60750-010-0', 'publisher': 'IOS Press', 'url': 'db/series/faia/faia196.html'}\n",
      "Record 11: {'type': 'proceedings', 'id': 'series/faia/2005-130', 'mdate': '2009-02-26', 'editor': 'Tomasz Szmuc', 'title': 'Software Engineering: Evolution and Emerging Technologies', 'booktitle': 'Software Engineering: Evolution and Emerging Technologies', 'volume': '130', 'series': 'Frontiers in Artificial Intelligence and Applications', 'year': '2005', 'isbn': '978-1-58603-559-4', 'publisher': 'IOS Press', 'url': 'db/series/faia/faia130.html'}\n",
      "Record 12: {'type': 'proceedings', 'id': 'series/gidiss/2022', 'mdate': '2023-11-17', 'editor': 'Klaus Wehrle', 'title': 'Ausgezeichnete Informatikdissertationen 2022.', 'publisher': 'GI', 'year': '2023', 'series': 'LNI', 'volume': 'D-23', 'ee': 'https://dl.gi.de/items/5ebfcf6a-f95f-49c1-ba15-cadc988f7d7b', 'url': 'db/series/gidiss/gidiss2022.html'}\n",
      "Record 13: {'type': 'proceedings', 'id': 'series/gidiss/2021', 'mdate': '2023-11-16', 'editor': 'Klaus Wehrle', 'title': 'Ausgezeichnete Informatikdissertationen 2021.', 'publisher': 'GI', 'year': '2022', 'series': 'LNI', 'volume': 'D-22', 'isbn': '978-3-88579-980-1', 'ee': 'https://dl.gi.de/handle/20.500.12116/39856', 'url': 'db/series/gidiss/gidiss2021.html'}\n",
      "Record 14: {'type': 'proceedings', 'id': 'series/sci/2015-589', 'mdate': '2021-10-14', 'editor': 'Maria Simi', 'title': 'Harmonization and Development of Resources and Tools for Italian Natural Language Processing within the PARLI Project', 'booktitle': 'Italian Natural Language Processing within the PARLI Project', 'year': '2015', 'publisher': 'Springer', 'series': 'Studies in Computational Intelligence', 'volume': '589', 'ee': 'https://doi.org/10.1007/978-3-319-14206-7', 'isbn': '978-3-319-14205-0', 'url': 'db/series/sci/sci589.html'}\n",
      "Record 15: {'type': 'proceedings', 'id': 'series/sci/2015-578', 'mdate': '2018-09-25', 'editor': 'Roger Y. Lee', 'title': 'Software Engineering Research, Management and Applications [selected papers from the 12th International Conference on Software Engineering Research, Management and Applications, SERA 2014, Kitakyushu, Japan, 2014]', 'year': '2015', 'publisher': 'Springer', 'series': 'Studies in Computational Intelligence', 'volume': '578', 'ee': 'https://doi.org/10.1007/978-3-319-11265-7', 'isbn': '978-3-319-11264-0', 'booktitle': 'Software Engineering Research, Management and Applications', 'url': 'db/series/sci/sci578.html'}\n",
      "Record 16: {'type': 'proceedings', 'id': 'series/sci/2015-596', 'mdate': '2021-09-07', 'editor': 'Valentin Robu', 'title': 'Next Frontier in Agent-Based Complex Automated Negotiation', 'booktitle': 'Next Frontier in Agent-Based Complex Automated Negotiation', 'publisher': 'Springer', 'year': '2015', 'series': 'Studies in Computational Intelligence', 'volume': '596', 'isbn': '978-4-431-55524-7', 'ee': 'https://doi.org/10.1007/978-4-431-55525-4', 'url': 'db/series/sci/sci596.html'}\n",
      "Record 17: {'type': 'proceedings', 'id': 'series/sci/2015-595', 'mdate': '2017-05-16', 'editor': 'Tadeusz Luba', 'title': 'Computational Intelligence and Efficiency in Engineering Systems', 'booktitle': 'Computational Intelligence and Efficiency in Engineering Systems', 'publisher': 'Springer', 'year': '2015', 'series': 'Studies in Computational Intelligence', 'volume': '595', 'isbn': '978-3-319-15719-1', 'ee': 'https://doi.org/10.1007/978-3-319-15720-7', 'url': 'db/series/sci/sci595.html'}\n",
      "Record 18: {'type': 'proceedings', 'id': 'series/sci/2015-585', 'mdate': '2020-01-28', 'editor': 'Xin-She Yang 0001', 'title': 'Recent Advances in Swarm Intelligence and Evolutionary Computation', 'booktitle': 'Recent Advances in Swarm Intelligence and Evolutionary Computation', 'year': '2015', 'publisher': 'Springer', 'series': 'Studies in Computational Intelligence', 'volume': '585', 'ee': 'https://doi.org/10.1007/978-3-319-13826-8', 'isbn': '978-3-319-13825-1', 'url': 'db/series/sci/sci585.html'}\n",
      "Record 19: {'type': 'proceedings', 'id': 'series/sci/2015-584', 'mdate': '2018-11-02', 'editor': 'Lakhmi C. Jain', 'title': 'Feature Selection for Data and Pattern Recognition', 'booktitle': 'Feature Selection for Data and Pattern Recognition', 'year': '2015', 'publisher': 'Springer', 'series': 'Studies in Computational Intelligence', 'volume': '584', 'ee': 'https://doi.org/10.1007/978-3-662-45620-0', 'isbn': '978-3-662-45619-4', 'url': 'db/series/sci/sci584.html'}\n",
      "Record 20: {'type': 'proceedings', 'id': 'series/sci/2015-594', 'mdate': '2018-08-23', 'editor': 'Damien Trentesaux', 'title': 'Service Orientation in Holonic and Multi-agent Manufacturing', 'booktitle': 'Service Orientation in Holonic and Multi-agent Manufacturing', 'publisher': 'Springer', 'year': '2015', 'series': 'Studies in Computational Intelligence', 'volume': '594', 'isbn': '978-3-319-15158-8', 'ee': 'https://doi.org/10.1007/978-3-319-15159-5', 'url': 'db/series/sci/sci594.html'}\n",
      "Record 21: {'type': 'proceedings', 'id': 'series/sci/2013-471', 'mdate': '2023-09-30', 'editor': 'Djamel Abdelkader Zighed', 'title': 'Advances in Knowledge Discovery and Management - Volume 3 [Best of EGC 2011, Brest, France].', 'year': '2013', 'publisher': 'Springer', 'series': 'Studies in Computational Intelligence', 'volume': '471', 'ee': 'https://www.wikidata.org/entity/Q57395171', 'isbn': '978-3-642-35854-8', 'booktitle': 'EGC (best of volume)', 'url': 'db/series/sci/sci471.html'}\n",
      "Record 22: {'type': 'proceedings', 'id': 'series/sci/2015-592', 'mdate': '2017-05-16', 'editor': 'Mrutyunjaya Panda', 'title': 'Multi-objective Swarm Intelligence - Theoretical Advances and Applications', 'booktitle': 'Multi-objective Swarm Intelligence', 'publisher': 'Springer', 'year': '2015', 'series': 'Studies in Computational Intelligence', 'volume': '592', 'isbn': '978-3-662-46308-6', 'ee': 'https://doi.org/10.1007/978-3-662-46309-3', 'url': 'db/series/sci/sci592.html'}\n",
      "Record 23: {'type': 'proceedings', 'id': 'series/ihis/2009hoo', 'mdate': '2018-11-14', 'editor': 'Rudi Studer', 'title': 'Handbook on Ontologies', 'publisher': 'Springer', 'year': '2009', 'isbn': '978-3-540-92673-3', 'ee': 'https://www.wikidata.org/entity/Q56840168', 'series': 'International Handbooks on Information Systems', 'url': 'db/series/ihis/hoo2009.html'}\n",
      "Record 24: {'type': 'proceedings', 'id': 'series/ihis/2015bpm1', 'mdate': '2017-05-16', 'editor': 'Michael Rosemann', 'title': 'Handbook on Business Process Management 1, Introduction, Methods, and Information Systems, 2nd Ed.', 'publisher': 'Springer', 'year': '2015', 'isbn': '978-3-642-45100-3', 'ee': 'https://doi.org/10.1007/978-3-642-45100-3', 'series': 'International Handbooks on Information Systems', 'url': 'db/series/ihis/bpm2015-1.html'}\n",
      "Record 25: {'type': 'proceedings', 'id': 'series/ihis/2015bpm2', 'mdate': '2017-05-16', 'editor': 'Michael Rosemann', 'title': 'Handbook on Business Process Management 2, Strategic Alignment, Governance, People and Culture, 2nd Ed.', 'publisher': 'Springer', 'year': '2015', 'isbn': '978-3-642-45103-4', 'ee': 'https://doi.org/10.1007/978-3-642-45103-4', 'series': 'International Handbooks on Information Systems', 'url': 'db/series/ihis/bpm2015-2.html'}\n",
      "Record 26: {'type': 'proceedings', 'id': 'series/ciss/2013-10', 'mdate': '2024-04-17', 'editor': 'Amit Sahai', 'title': 'Secure Multi-Party Computation', 'booktitle': 'Secure Multi-Party Computation', 'publisher': 'IOS Press', 'year': '2013', 'series': 'Cryptology and Information Security Series', 'volume': '10', 'isbn': '978-1-61499-169-4', 'url': 'db/series/ciss/ciss10.html'}\n",
      "Record 27: {'type': 'proceedings', 'id': 'series/ciss/2009-2', 'mdate': '2019-04-11', 'editor': 'Gregory Neven', 'title': 'Identity-Based Cryptography', 'publisher': 'IOS Press', 'year': '2009', 'series': 'Cryptology and Information Security Series', 'volume': '2', 'isbn': '978-1-58603-947-9', 'ee': 'http://ebooks.iospress.nl/volume/identity-based-cryptography', 'url': 'db/series/ciss/ciss2.html'}\n",
      "Record 28: {'type': 'proceedings', 'id': 'conf/dac/1995', 'mdate': '2011-11-30', 'editor': 'Bryan Preas', 'title': 'Proceedings of the 32st Conference on Design Automation, San Francisco, California, USA, Moscone Center, June 12-16, 1995.', 'booktitle': 'DAC', 'publisher': 'ACM Press', 'year': '1995', 'isbn': '0-89791-725-1', 'url': 'db/conf/dac/dac95.html', 'ee': 'http://dl.acm.org/citation.cfm?id=217474'}\n",
      "Record 29: {'type': 'proceedings', 'id': 'conf/dac/2010', 'mdate': '2011-11-30', 'editor': 'Sachin S. Sapatnekar', 'title': 'Proceedings of the 47th Design Automation Conference, DAC 2010, Anaheim, California, USA, July 13-18, 2010', 'booktitle': 'DAC', 'publisher': 'ACM', 'isbn': '978-1-4503-0002-5', 'year': '2010', 'ee': 'http://dl.acm.org/citation.cfm?id=1837274', 'url': 'db/conf/dac/dac2010.html'}\n",
      "Record 30: {'type': 'proceedings', 'id': 'conf/dac/2004', 'mdate': '2011-11-30', 'editor': 'Andrew B. Kahng', 'title': 'Proceedings of the 41th Design Automation Conference, DAC 2004, San Diego, CA, USA, June 7-11, 2004', 'booktitle': 'DAC', 'publisher': 'ACM', 'year': '2004', 'isbn': '1-58113-828-8', 'ee': 'http://dl.acm.org/citation.cfm?id=996566', 'url': 'db/conf/dac/dac2004.html'}\n",
      "Found and printed 30 records of 'proceedings'\n"
     ]
    }
   ],
   "source": [
    "class Find_K_Records(xml.sax.ContentHandler):\n",
    "    def __init__(self, target_type, k):\n",
    "        super().__init__()\n",
    "        self.current_element = \"\"\n",
    "        self.current_data = {}\n",
    "        self.record_count = 0  # 用于计数已找到的记录\n",
    "        self.found_target = False  # 标记是否找到目标记录\n",
    "        self.target_type = target_type\n",
    "\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.current_element = tag\n",
    "        # 如果找到incollection类型的记录，则初始化记录数据\n",
    "        if tag == self.target_type:\n",
    "            self.current_data = {\"type\": tag, \"id\": attributes[\"key\"], \"mdate\": attributes[\"mdate\"]}\n",
    "            self.found_target = True\n",
    "\n",
    "    def endElement(self, tag):\n",
    "        # 在incollection的记录结束时打印该记录的所有元素\n",
    "        if tag == self.target_type and self.found_target:\n",
    "            self.record_count += 1\n",
    "            print(f\"Record {self.record_count}: {self.current_data}\")\n",
    "            self.found_target = False  # 重置标志\n",
    "\n",
    "            # 如果只需要查看一条记录，这里可以终止解析\n",
    "            if self.record_count == k:\n",
    "                raise xml.sax.SAXException(\"Found target, stop parsing\")  # 通过抛出异常停止解析\n",
    "\n",
    "        # 在找到incollection的记录时，将当前元素的内容添加到记录中\n",
    "        if self.found_target and self.current_element:\n",
    "            self.current_data[self.current_element] = self.content\n",
    "\n",
    "    def characters(self, content):\n",
    "        # 记录当前元素的内容\n",
    "        self.content = content.strip()\n",
    "\n",
    "# 初始化解析器和处理器\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "target_type = \"proceedings\"\n",
    "k = 30\n",
    "handler = Find_K_Records(target_type=target_type, k=k)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# 解析XML文件\n",
    "xml_file = \"../ntu_sd6103_team_project_data/dblp.xml\"\n",
    "try:\n",
    "    parser.parse(xml_file)\n",
    "except xml.sax.SAXException:\n",
    "    print(f\"Found and printed {k} records of '{target_type}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incollection record:**\n",
    "- type: incollection (no null)\n",
    "- id: one and only (no null)\n",
    "- date: date (no null)\n",
    "- author\n",
    "- title: incollection title\n",
    "- year: year\n",
    "- booktitle:\n",
    "- ee\n",
    "- url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class incollectionParser(xml.sax.ContentHandler):\n",
    "    def __init__(self, batch_size=5000):\n",
    "        super().__init__()\n",
    "        self.current_element = \"\"\n",
    "        self.current_data = {}\n",
    "        self.records = []\n",
    "        self.authors = []\n",
    "        self.orcid = []\n",
    "        self.batch_size = batch_size\n",
    "        self.file_count = 1\n",
    "\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.current_element = tag\n",
    "        if tag == 'incollection':\n",
    "            # Reset for each new publication record\n",
    "            # print(self.current_data)\n",
    "            # print(f\"*****Processing {tag}...******\")\n",
    "            self.current_data = {\n",
    "                \"type\": tag,\n",
    "                \"id\": attributes[\"key\"],\n",
    "                \"date\": attributes[\"mdate\"],\n",
    "                \"title\": \"\",\n",
    "                \"pages\": \"\",\n",
    "                \"year\": \"\",\n",
    "                \"booktitle\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"ee\": \"\",\n",
    "                \"url\": \"\"\n",
    "            }\n",
    "            self.authors = []\n",
    "\n",
    "    def endElement(self, tag):\n",
    "        if tag in [\"incollection\"]:\n",
    "            # Add authors as a comma-separated string\n",
    "            self.current_data[\"authors\"] = \", \".join(self.authors)\n",
    "            self.records.append(self.current_data)\n",
    "            if len(self.records) >= self.batch_size:\n",
    "                self.save_to_csv()\n",
    "                self.records = []  # Reset records\n",
    "        elif tag == \"author\":\n",
    "            # Append author to authors list\n",
    "            self.authors.append(self.content)\n",
    "            # print(f\"author: {self.content}\")\n",
    "        elif tag in self.current_data:\n",
    "            # Save the content to the current field\n",
    "            self.current_data[tag] = self.content\n",
    "            # print(f\"{tag}: {self.content}\")\n",
    "\n",
    "    def characters(self, content):\n",
    "        self.content = content.strip()\n",
    "\n",
    "    def save_to_csv(self):\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files/incollection'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files/incollection')\n",
    "        csv_file = f\"../ntu_sd6103_team_project_data/csv_files/incollection/dplr_incollection_part_{self.file_count}.csv\"\n",
    "        with open(csv_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.records[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.records)\n",
    "        print(f\"Saving batch {self.file_count} to {csv_file}\")\n",
    "        self.file_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "phdthesis record:\n",
    "\n",
    "- type: phdthesis\n",
    "- id: \n",
    "- date\n",
    "- author\n",
    "- title\n",
    "- publisher\n",
    "- year\n",
    "- series\n",
    "- volume\n",
    "- pages\n",
    "- school\n",
    "- isbn\n",
    "- ee\n",
    "- note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class phdthesisParser(xml.sax.ContentHandler):\n",
    "    def __init__(self, batch_size=5000):\n",
    "        super().__init__()\n",
    "        self.current_element = \"\"\n",
    "        self.current_data = {}\n",
    "        self.records = []\n",
    "        self.authors = []\n",
    "        self.batch_size = batch_size\n",
    "        self.file_count = 1\n",
    "\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.current_element = tag\n",
    "        if tag == 'phdthesis':\n",
    "            # Reset for each new publication record\n",
    "            # print(self.current_data)\n",
    "            # print(f\"*****Processing {tag}...******\")\n",
    "            self.current_data = {\n",
    "                \"type\": tag,\n",
    "                \"id\": attributes[\"key\"],\n",
    "                \"date\": attributes[\"mdate\"],\n",
    "                \"title\": \"\",\n",
    "                \"pages\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"year\": \"\",\n",
    "                \"series\": \"\",\n",
    "                \"volume\": \"\",\n",
    "                \"school\": \"\",\n",
    "                \"isbn\": \"\",\n",
    "                \"ee\": \"\"\n",
    "            }\n",
    "            self.authors = []\n",
    "\n",
    "    def endElement(self, tag):\n",
    "        if tag in [\"phdthesis\"]:\n",
    "            # Add authors as a comma-separated string\n",
    "            self.current_data[\"authors\"] = \", \".join(self.authors)\n",
    "            self.records.append(self.current_data)\n",
    "            if len(self.records) >= self.batch_size:\n",
    "                self.save_to_csv()\n",
    "                self.records = []  # Reset records\n",
    "        elif tag == \"author\":\n",
    "            # Append author to authors list\n",
    "            self.authors.append(self.content)\n",
    "            # print(f\"author: {self.content}\")\n",
    "        elif tag in self.current_data:\n",
    "            # Save the content to the current field\n",
    "            self.current_data[tag] = self.content\n",
    "            # print(f\"{tag}: {self.content}\")\n",
    "\n",
    "    def characters(self, content):\n",
    "        self.content = content.strip()\n",
    "\n",
    "    def save_to_csv(self):\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files/phdthesis'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files/phdthesis')\n",
    "        csv_file = f\"../ntu_sd6103_team_project_data/csv_files/phdthesis/dplr_phdthesis_part_{self.file_count}.csv\"\n",
    "        with open(csv_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.records[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.records)\n",
    "        print(f\"Saving batch {self.file_count} to {csv_file}\")\n",
    "        self.file_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class masterthesisParser(xml.sax.ContentHandler):\n",
    "    def __init__(self, batch_size=5000):\n",
    "        super().__init__()\n",
    "        self.current_element = \"\"\n",
    "        self.current_data = {}\n",
    "        self.records = []\n",
    "        self.authors = []\n",
    "        self.batch_size = batch_size\n",
    "        self.file_count = 1\n",
    "\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.current_element = tag\n",
    "        if tag == 'masterthesis':\n",
    "            # Reset for each new publication record\n",
    "            # print(self.current_data)\n",
    "            print(f\"*****Processing {tag}...******\")\n",
    "            self.current_data = {\n",
    "                \"type\": tag,\n",
    "                \"id\": attributes[\"key\"],\n",
    "                \"date\": attributes[\"mdate\"],\n",
    "                \"title\": \"\",\n",
    "                \"year\": \"\",\n",
    "                \"school\": \"\",\n",
    "                \"ee\": \"\"\n",
    "            }\n",
    "            self.authors = []\n",
    "\n",
    "    def endElement(self, tag):\n",
    "        if tag in [\"masterthesis\"]:\n",
    "            # Add authors as a comma-separated string\n",
    "            self.current_data[\"authors\"] = \", \".join(self.authors)\n",
    "            self.records.append(self.current_data)\n",
    "            if len(self.records) >= self.batch_size:\n",
    "                self.save_to_csv()\n",
    "                self.records = []  # Reset records\n",
    "        elif tag == \"author\":\n",
    "            # Append author to authors list\n",
    "            self.authors.append(self.content)\n",
    "            # print(f\"author: {self.content}\")\n",
    "        elif tag in self.current_data:\n",
    "            # Save the content to the current field\n",
    "            self.current_data[tag] = self.content\n",
    "            # print(f\"{tag}: {self.content}\")\n",
    "\n",
    "    def characters(self, content):\n",
    "        self.content = content.strip()\n",
    "\n",
    "    def save_to_csv(self):\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files/masterthesis'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files/masterthesis')\n",
    "        csv_file = f\"../ntu_sd6103_team_project_data/csv_files/masterthesis/dplr_masterthesis_part_{self.file_count}.csv\"\n",
    "        with open(csv_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.records[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.records)\n",
    "        print(f\"Saving batch {self.file_count} to {csv_file}\")\n",
    "        self.file_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "www records:\n",
    "\n",
    "- type\n",
    "- id\n",
    "- date\n",
    "- title\n",
    "- pages\n",
    "- note\n",
    "- url\n",
    "- authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wwwParser(xml.sax.ContentHandler):\n",
    "    def __init__(self, batch_size=5000):\n",
    "        super().__init__()\n",
    "        self.current_element = \"\"\n",
    "        self.current_data = {}\n",
    "        self.records = []\n",
    "        self.authors = []\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        self.file_count = 1  # csv No.\n",
    "\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.current_element = tag\n",
    "        if tag == 'www':\n",
    "            # Reset for each new publication record\n",
    "            # print(self.current_data)\n",
    "            # print(f\"*****Processing {tag}...******\")\n",
    "            self.current_data = {\n",
    "                \"type\": tag,\n",
    "                \"id\": attributes[\"key\"],\n",
    "                \"date\": attributes[\"mdate\"],\n",
    "                \"title\": \"\",\n",
    "                \"pages\": \"\",\n",
    "                \"note\": \"\",\n",
    "                \"url\": \"\"\n",
    "            }\n",
    "            self.authors = []\n",
    "\n",
    "    def endElement(self, tag):\n",
    "        if tag in [\"www\"]:\n",
    "            # Add authors as a comma-separated string\n",
    "            self.current_data[\"authors\"] = \", \".join(self.authors)\n",
    "            self.records.append(self.current_data)\n",
    "            if len(self.records) >= self.batch_size:\n",
    "                self.save_to_csv()\n",
    "                self.records = []  # Reset records\n",
    "        elif tag == \"author\":\n",
    "            # Append author to authors list\n",
    "            self.authors.append(self.content)\n",
    "            # print(f\"author: {self.content}\")\n",
    "        elif tag in self.current_data:\n",
    "            # Save the content to the current field\n",
    "            self.current_data[tag] = self.content\n",
    "            # print(f\"{tag}: {self.content}\")\n",
    "\n",
    "    def characters(self, content):\n",
    "        self.content = content.strip()\n",
    "    \n",
    "    def save_to_csv(self):\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files/www'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files/www')\n",
    "        csv_file = f\"../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_{self.file_count}.csv\"\n",
    "        with open(csv_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.records[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.records)\n",
    "        print(f\"Saving batch {self.file_count} to {csv_file}\")\n",
    "        self.file_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data record:\n",
    "\n",
    "- type,\n",
    "- id,\n",
    "- date,\n",
    "- title,\n",
    "- pages,\n",
    "- publisher,\n",
    "- year,\n",
    "- month,\n",
    "- ee,\n",
    "- stream,\n",
    "- rel,\n",
    "- authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataParser(xml.sax.ContentHandler):\n",
    "    def __init__(self, batch_size=5000):\n",
    "        super().__init__()\n",
    "        self.current_element = \"\"\n",
    "        self.current_data = {}\n",
    "        self.records = []\n",
    "        self.authors = []\n",
    "        self.batch_size = batch_size  # batch size\n",
    "        self.file_count = 1  # csv No.\n",
    "\n",
    "    def startElement(self, tag, attributes):\n",
    "        self.current_element = tag\n",
    "        if tag == 'data':\n",
    "            # Reset for each new publication record\n",
    "            # print(self.current_data)\n",
    "            # print(f\"*****Processing {tag}...******\")\n",
    "            self.current_data = {\n",
    "                \"type\": tag,\n",
    "                \"id\": attributes[\"key\"],\n",
    "                \"date\": attributes[\"mdate\"],\n",
    "                \"title\": \"\",\n",
    "                \"pages\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"year\": \"\",\n",
    "                \"month\": \"\",\n",
    "                \"ee\": \"\",\n",
    "                \"publisher\": \"\",\n",
    "                \"stream\": \"\",\n",
    "                \"rel\": \"\"\n",
    "            }\n",
    "            self.authors = []\n",
    "\n",
    "    def endElement(self, tag):\n",
    "        if tag in [\"data\"]:\n",
    "            # Add authors as a comma-separated string\n",
    "            self.current_data[\"authors\"] = \", \".join(self.authors)\n",
    "            self.records.append(self.current_data)\n",
    "            if len(self.records) >= self.batch_size:\n",
    "                self.save_to_csv()\n",
    "                self.records = []  # Reset records\n",
    "        elif tag == \"author\":\n",
    "            # Append author to authors list\n",
    "            self.authors.append(self.content)\n",
    "            # print(f\"author: {self.content}\")\n",
    "        elif tag in self.current_data:\n",
    "            # Save the content to the current field\n",
    "            self.current_data[tag] = self.content\n",
    "            # print(f\"{tag}: {self.content}\")\n",
    "\n",
    "    def characters(self, content):\n",
    "        self.content = content.strip()\n",
    "    \n",
    "    def save_to_csv(self):\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files')\n",
    "        if not os.path.exists('../ntu_sd6103_team_project_data/csv_files/data'):\n",
    "            os.mkdir('../ntu_sd6103_team_project_data/csv_files/data')\n",
    "        csv_file = f\"../ntu_sd6103_team_project_data/csv_files/data/dplr_data_part_{self.file_count}.csv\"\n",
    "        with open(csv_file, \"w\", newline='', encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=self.records[0].keys())\n",
    "            writer.writeheader()\n",
    "            writer.writerows(self.records)\n",
    "        print(f\"Saving batch {self.file_count} to {csv_file}\")\n",
    "        self.file_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the last batch...\n",
      "Saving batch 1 to ../ntu_sd6103_team_project_data/csv_files/incollection/dplr_incollection_part_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parser and handler\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "handler = incollectionParser(batch_size=5000000)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# Parse the XML file\n",
    "xml_file = \"../ntu_sd6103_team_project_data/dblp.xml\"\n",
    "parser.parse(xml_file)\n",
    "\n",
    "if handler.records:\n",
    "    print(\"Saving the last batch...\")\n",
    "    handler.save_to_csv()   # Save the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the last batch...\n",
      "Saving batch 1 to ../ntu_sd6103_team_project_data/csv_files/phdthesis/dplr_phdthesis_part_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parser and handler\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "handler = phdthesisParser(batch_size=50000000)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# Parse the XML file\n",
    "xml_file = \"../ntu_sd6103_team_project_data/dblp.xml\"\n",
    "parser.parse(xml_file)\n",
    "\n",
    "if handler.records:\n",
    "    print(\"Saving the last batch...\")\n",
    "    handler.save_to_csv()   # Save the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parser and handler\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "handler = masterthesisParser(batch_size=500000)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# Parse the XML file\n",
    "xml_file = \"../ntu_sd6103_team_project_data/dblp.xml\"\n",
    "parser.parse(xml_file)\n",
    "\n",
    "if handler.records:\n",
    "    print(\"Saving the last batch...\")\n",
    "    handler.save_to_csv()   # Save the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving batch 1 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_1.csv\n",
      "Saving batch 2 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_2.csv\n",
      "Saving batch 3 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_3.csv\n",
      "Saving batch 4 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_4.csv\n",
      "Saving batch 5 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_5.csv\n",
      "Saving batch 6 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_6.csv\n",
      "Saving batch 7 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_7.csv\n",
      "Saving the last batch...\n",
      "Saving batch 8 to ../ntu_sd6103_team_project_data/csv_files/www/dplr_www_part_8.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parser and handler\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "handler = wwwParser(batch_size=500000)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# Parse the XML file\n",
    "xml_file = \"../ntu_sd6103_team_project_data/dblp.xml\"\n",
    "parser.parse(xml_file)\n",
    "\n",
    "if handler.records:\n",
    "    print(\"Saving the last batch...\")\n",
    "    handler.save_to_csv()   # Save the last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving the last batch...\n",
      "Saving batch 1 to ../ntu_sd6103_team_project_data/csv_files/data/dplr_data_part_1.csv\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parser and handler\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setFeature(xml.sax.handler.feature_namespaces, 0)\n",
    "handler = dataParser(batch_size=500000)\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "# Parse the XML file\n",
    "xml_file = \"../ntu_sd6103_team_project_data/dblp.xml\"\n",
    "parser.parse(xml_file)\n",
    "\n",
    "if handler.records:\n",
    "    print(\"Saving the last batch...\")\n",
    "    handler.save_to_csv()   # Save the last batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datasystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
