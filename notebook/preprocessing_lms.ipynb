{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current working directory: C:\\Users\\jibin-cmii\\ntu_sd6103_data_systems_team_project-main\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if os.getcwd().endswith('notebook'):\n",
    "    os.chdir('..')\n",
    "print(f'current working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "pub_csv_path = r\"F:\\MS\\1119\\parsed_data\\publications.csv\"\n",
    "author_csv_path = r\"F:\\MS\\1119\\parsed_data\\authors.csv\"\n",
    "author_pub_csv_path = r\"F:\\MS\\1119\\parsed_data\\author_publications.csv\"\n",
    "jrnls_csv_path = r\"F:\\MS\\1119\\parsed_data\\journals.csv\"\n",
    "conf_csv_path = r\"F:\\MS\\1119\\parsed_data\\conferences.csv\"\n",
    "editors_csv_path = r\"F:\\MS\\1119\\parsed_data\\editors.csv\"\n",
    "publisher_csv_path = r\"F:\\MS\\1119\\parsed_data\\publishers.csv\"\n",
    "pub_df = pd.read_csv(pub_csv_path)\n",
    "au_pub_df = pd.read_csv(author_pub_csv_path)\n",
    "au_df = pd.read_csv(author_csv_path)\n",
    "jrnls_df = pd.read_csv(jrnls_csv_path)\n",
    "conf_df = pd.read_csv(conf_csv_path)\n",
    "editors_df = pd.read_csv(editors_csv_path)\n",
    "publisher_df = pd.read_csv(publisher_csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['type', 'pub_id', 'mdate', 'year', 'key_source', 'key_name', 'volume', 'publisher', 'journal', 'number', 'url', 'ee']\n",
      "(11179600, 12)\n",
      "------------------------------------\n",
      "['author_id', 'author_name', 'pub_id']\n",
      "(28556780, 3)\n",
      "------------------------------------\n",
      "['author_id', 'name']\n",
      "(28556780, 2)\n",
      "------------------------------------\n",
      "['journal_id', 'name']\n",
      "(3669449, 2)\n",
      "------------------------------------\n",
      "['conf_id', 'name']\n",
      "(0, 2)\n",
      "------------------------------------\n",
      "['editor_id', 'name']\n",
      "(152730, 2)\n",
      "------------------------------------\n",
      "['publisher_id', 'name']\n",
      "(93944, 2)\n",
      "------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for df in [pub_df, au_pub_df, au_df, jrnls_df, conf_df, editors_df, publisher_df]:\n",
    "    # print(df.info())\n",
    "    #print(df.head())\n",
    "    print(df.columns.tolist())\n",
    "    print(df.shape)\n",
    "    print('------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conf_id</th>\n",
       "      <th>pub_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>58289</th>\n",
       "      <td>0</td>\n",
       "      <td>dac1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58331</th>\n",
       "      <td>1</td>\n",
       "      <td>dac2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58449</th>\n",
       "      <td>2</td>\n",
       "      <td>dac2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58583</th>\n",
       "      <td>3</td>\n",
       "      <td>dac1964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58801</th>\n",
       "      <td>4</td>\n",
       "      <td>dac1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678517</th>\n",
       "      <td>59245</td>\n",
       "      <td>sgiot2021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678554</th>\n",
       "      <td>59246</td>\n",
       "      <td>sgiot2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678575</th>\n",
       "      <td>59247</td>\n",
       "      <td>sgiot2020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678581</th>\n",
       "      <td>59248</td>\n",
       "      <td>sgiot2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3678591</th>\n",
       "      <td>59249</td>\n",
       "      <td>sgiot2019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         conf_id     pub_id\n",
       "58289          0    dac1995\n",
       "58331          1    dac2010\n",
       "58449          2    dac2004\n",
       "58583          3    dac1964\n",
       "58801          4    dac1986\n",
       "...          ...        ...\n",
       "3678517    59245  sgiot2021\n",
       "3678554    59246  sgiot2023\n",
       "3678575    59247  sgiot2020\n",
       "3678581    59248  sgiot2018\n",
       "3678591    59249  sgiot2019\n",
       "\n",
       "[59250 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# pub_df\n",
    "\n",
    "# conference list\n",
    "pub_df[(pub_df['key_source'] == 'conf') & (pub_df['type'] == 'proceedings')]\n",
    "\n",
    "# 定义一个函数来转换 pub_id\n",
    "def transform_pub_id(pub_id):\n",
    "    # 使用正则表达式匹配并提取 dac 和 1995\n",
    "    match = re.match(r'(\\w+)/(\\d+)', pub_id)\n",
    "    if match:\n",
    "        return f\"{match.group(1)}{match.group(2)}\"\n",
    "    else:\n",
    "        return pub_id  # 如果格式不符合预期，保持原样\n",
    "\n",
    "# 应用转换函数到 pub_id 列\n",
    "conf_df['conf_id'] = conf_df['key_name'].apply(transform_pub_id)\n",
    "conf_df['name']=conf_df['publisher']\n",
    "\n",
    "# 打印前几行查看结果\n",
    "conf_df\n",
    "\n",
    "conf_df['conf_id'] = range(0, len(conf_df))\n",
    "conf_df_selected = conf_df[['conf_id', 'pub_id']]\n",
    "conf_df_selected.rename(columns={'key_name': 'name'})\n",
    "conf_df_selected.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\conferences.csv', index=False)\n",
    "conf_df_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "print('-1')\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\author_publications.csv')\n",
    "print('0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been split and saved.\n"
     ]
    }
   ],
   "source": [
    "# 筛选出 author_id 为空的行\n",
    "df_null = df[df['author_id'].isna()]\n",
    "\n",
    "# 筛选出 author_id 不为空的行\n",
    "df_not_null = df[df['author_id'].notna()]\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "df_null.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\author_publications_null.csv', index=False)\n",
    "df_not_null.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\author_publications_not_null.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been split and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('-1')\n",
    "# 读取CSV文件\n",
    "author_df = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\authors.csv')\n",
    "print('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV files have been split and saved.\n"
     ]
    }
   ],
   "source": [
    "# 筛选出 author_id 为空的行\n",
    "author_df_null = author_df[df['author_id'].isna()]\n",
    "\n",
    "# 筛选出 author_id 不为空的行\n",
    "author_df_not_null = author_df[df['author_id'].notna()]\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "author_df_null.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\author_null.csv', index=False)\n",
    "author_df_not_null.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\author_not_null.csv', index=False)\n",
    "\n",
    "print(\"CSV files have been split and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22806838, 2)\n",
      "(3630718, 2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-babd4c730b30>:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_unique['author_id'] = df_unique.reset_index(drop=True).index.map(generate_id)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "author_df_not_null_from_csv=pd.read_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\author_null.csv')\n",
    "# 对 name 列进行去重\n",
    "print(author_df_not_null_from_csv.shape)\n",
    "df_unique = author_df_not_null_from_csv.drop_duplicates(subset=['name'])\n",
    "print(df_unique.shape)\n",
    "\n",
    "# 生成编号\n",
    "def generate_id(index):\n",
    "    # 计算 yyyy 和 XXXX\n",
    "    xxxx = (index + 1) % 10000\n",
    "    yyyy = (index // 10000) + 1\n",
    "    return f\"0000-0004-{str(yyyy).zfill(4)}-{str(xxxx).zfill(4)}\"\n",
    "\n",
    "# 为去重后的 name 列生成编号\n",
    "df_unique['author_id'] = df_unique.reset_index(drop=True).index.map(generate_id)\n",
    "\n",
    "# 保存处理后的CSV文件\n",
    "df_unique.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\1author_publications_unique.csv', index=False)\n",
    "\n",
    "print(\"CSV file has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 读取CSV文件\n",
    "df1 = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\1author_unique.csv')\n",
    "df2 = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\author_publications_null.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "author_publications_null_updated CSV file has been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将df1保存到字典中，key是name，value是author_id\n",
    "author_id_map = dict(zip(df1['name'], df1['author_id']))\n",
    "\n",
    "# 遍历df2，更新author_id列\n",
    "df2['author_id'] = df2['author_name'].map(author_id_map)\n",
    "\n",
    "# 保存处理后的CSV文件\n",
    "df2.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\2author_publications_null_updated.csv', index=False)\n",
    "\n",
    "print(\"author_publications_null_updated CSV file has been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has 合并 been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df1 = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\1author_not_null.csv')\n",
    "df2 = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\1author_unique.csv')\n",
    "\n",
    "# 合并两个DataFrame\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 去重：所有列的值都相等的行\n",
    "unique_df = combined_df.drop_duplicates()\n",
    "\n",
    "# 保存处理后的CSV文件\n",
    "unique_df.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\authors.csv', index=False)\n",
    "\n",
    "print(\"CSV file has 合并 been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file has 合并 been processed and saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取CSV文件\n",
    "df1 = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\2author_publications_not_null.csv')\n",
    "df2 = pd.read_csv(r'F:\\MS\\1119\\parsed_data\\result\\process\\2author_publications_null_updated.csv')\n",
    "\n",
    "# 合并两个DataFrame\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# 去重：所有列的值都相等的行\n",
    "unique_df = combined_df.drop_duplicates()\n",
    "\n",
    "# 保存处理后的CSV文件\n",
    "unique_df.to_csv(r'F:\\MS\\1119\\parsed_data\\result\\author_publications.csv', index=False)\n",
    "\n",
    "print(\"CSV file has 合并 been processed and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
